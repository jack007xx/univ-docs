# 3章
## シナプス結合
興奮と抑制はNaとかClイオンの電位による。
筋肉と違うんだねって感じ。
パルスを送る。

## 神経回路の成長
1歳くらいまでで急激に増加する。

## McCulloch-Pittsのモデル
入力に重みつけて足し合わせて関数通すだけ？
閾値超えたら発火！

入出力は非線形になる。
論理式とかもエミュできる。(何のイミがある？)

### シグモイド関数
微分可能な関数で処理したい。
S字っぽいやつ。

最近ではゼロ以上で直線な関数が使われるらしい。(ランプ関数)

## どう学習する？
### サイコロで3連続1が出たら
ディープラーニングでは1の確率が他の目より高いという修正を行う。
(確率密度関数を変える)
ベイズ論的な考え。

頻度論的には元になる分布を仮定しているので途中で修正しない。

ネコっぽい画像をたくさん食わせたらネコによっていく。

### ヘッブの学習則
重みの漸化式みたいのを考える。
入力がより関係していて，発火に寄与した場合にどんどん重みを強くする。

ベクトル化して複数入出力にする。

## 多層パーセプトロン
近似，パターン分類，連想記憶。
シナプスの結合→ヘッブ則

### 単順パーセプトロン
単順パーセプトロンは小脳に同様の回路がある。
教師信号との誤差に基づいて学習

感覚層→連合層→反応層で分かれる。
感→連は固定変換。
OUTPUTにたいして教師信号との誤差を修正するような重み調整をする。

#### 学習の収束
変換したあとのXが線形分離可能なら，有限回の学習で識別できる。
つまり，各要素が超平面で分けることができるならば，それに基づいてグルーピングできる。

でも，これだと超平面で分けられないグニャグニャの問題が識別できない。

連合層の出力に対して教師信号があれば，複雑な問題を解決できる。
(固定変換の部分を柔軟にしたい)

#### Adaline
線形分離可能な問題を解けるアナログ素子。
並列にしてアンドをとるとちょっと複雑な(XORとか)にも対応できる。
2直線を引くと分割区域は増えるのでアタリマエである。
